{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0441f00c",
   "metadata": {},
   "source": [
    "Set up the running environment and file path for the following data:\n",
    "    NSD, \n",
    "    NSD_beta, \n",
    "    exp_design files, \n",
    "    COCO annotation files(records the detailed information of the image), \n",
    "    COCO caption files(records the textual description of the image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7174b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "cap_lenght = \"s\"\n",
    "os.chdir(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "nsd_root = \"/data/public/wbc/NSDdataset/\"  \n",
    "stim_root = nsd_root + \"nsddata_stimuli/stimuli/nsd/\"  # Contains color natural scene images used in NSD experiments\n",
    "beta_root = nsd_root + \"nsddata_betas/ppdata/\" \n",
    "mask_root = nsd_root + \"nsddata/ppdata/\"\n",
    "exp_design_file = nsd_root + \"nsddata/experiments/nsd/nsd_expdesign.mat\" \n",
    "exp_design = loadmat(exp_design_file)\n",
    "basic_cnt    = exp_design['basiccnt']  # \n",
    "shared_idx   = exp_design['sharedix']  # Sort index for shared images\n",
    "subject_idx  = exp_design['subjectim']   # 8 x 10000, The nsd ID corresponding to each trial (stem) is the same for the first 8x1000 and is the same as the sharefix\n",
    "trial_order  = exp_design['masterordering']  \n",
    "stim_pattern = exp_design['stimpattern']  # It consists of 40 sessions x 12 runs x 75 experiments. The element is 0/1, indicating the actual occurrence time of the stimulus test\n",
    "nsd_stiminfo_file = nsd_root + 'nsddata/experiments/nsd/nsd_stim_info_merged.pkl'\n",
    "stiminfo = pd.read_pickle(nsd_stiminfo_file)   # Coco ID and some information corresponding to each stimulus image \n",
    "\n",
    "annDir = '/data/public/wbc//NSDdataset/coco/'  \n",
    "imgDir = annDir + 'trainval2017/' # combined folder with train2017 and val2017 png masks/\n",
    "instances_trn_annFile = annDir + 'annotations_trainval2017/annotations/instances_train2017.json'  # annFile\n",
    "instances_val_annFile = annDir + 'annotations_trainval2017/annotations/instances_val2017.json'  # annFile\n",
    "instance_trn_capFile = annDir + 'annotations_trainval2017/annotations/captions_train2017.json'  # capFile\n",
    "instance_val_capFile = annDir + 'annotations_trainval2017/annotations/captions_val2017.json'  # capFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e477010",
   "metadata": {},
   "source": [
    "Match the images used in the NSD experiment with the COCO dataset to obtain information such as their categories and textual descriptions.\n",
    "\n",
    "Due to the fact that the Coco dataset provides 5 sentences for each image, long sentences can exceed the maximum length limit of the model. Therefore, we adopt the shortest sentence to minimize the loss of descriptive information as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe519d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco \n",
    "instance_trn_cap = open(instance_trn_capFile, 'r')\n",
    "dataset = json.load(instance_trn_cap)\n",
    "imgIdToCaps = defaultdict(list)\n",
    "imgIdToImg = defaultdict(list)\n",
    "if 'annotations' in dataset:\n",
    "    for ann in dataset['annotations']:\n",
    "        imgIdToCaps[ann['image_id']].append(ann)\n",
    "        \n",
    "if 'images' in dataset:\n",
    "    for ann in dataset['images']:\n",
    "        imgIdToImg[ann['id']].append(ann)\n",
    "\n",
    "dataset = dict()\n",
    "instance_val_cap = open(instance_val_capFile, 'r')\n",
    "dataset = json.load(instance_val_cap)\n",
    "\n",
    "if 'annotations' in dataset:\n",
    "    for ann in dataset['annotations']:\n",
    "        imgIdToCaps[ann['image_id']].append(ann)\n",
    "\n",
    "if 'images' in dataset:\n",
    "    for ann in dataset['images']:\n",
    "        imgIdToImg[ann['id']].append(ann)\n",
    "        \n",
    "cocoMap = np.ones(shape=(73000), dtype=int) * -1\n",
    "for j in range(len(subject_idx)):  \n",
    "    cocoId = np.array(stiminfo['cocoId'])[stiminfo['subject%d'%(j+1)].astype(bool)]  \n",
    "    nsdId = np.array(stiminfo['nsdId'])[stiminfo['subject%d'%(j+1)].astype(bool)]   \n",
    "    cocoMap[nsdId] = cocoId\n",
    "\n",
    "\n",
    "max_str_length = 0\n",
    "imgIdToCaps_simple = {}  # {cocoid:(str)cap}\n",
    "for k in imgIdToCaps.keys():\n",
    "    c_item_list = imgIdToCaps[k]\n",
    "    cap_list = []\n",
    "    # Traverse 5 sentences to extract a single image\n",
    "    for c_item in c_item_list:\n",
    "        cap = c_item['caption']\n",
    "        cap_list.append(cap)\n",
    "    cap_list.sort(reverse=True)\n",
    "    cap_s = cap_list[0].rstrip() #Shortest\n",
    "    cap_m = cap_list[2].rstrip() # medium\n",
    "    cap_l = cap_list[-1].rstrip() # longest\n",
    "    cap = cap_s\n",
    "    imgIdToCaps_simple[k] = cap  # {cocoid:(str)cap}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23908895",
   "metadata": {},
   "source": [
    "Load a ViLT model based on Coco dataset fine-tuning and use it for encoding and processing of text and image information. Using a pooler_ Output as the result of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c9060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/public/wbc/data_analysis/nsddata_voxel_analysis/things/ViLT/vilt-b32-finetuned-coco were not used when initializing ViltModel: ['rank_output.weight', 'rank_output.bias']\n",
      "- This IS expected if you are initializing ViltModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViltModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "360it [00:19, 24.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13880it [12:47, 14.12it/s]"
     ]
    }
   ],
   "source": [
    "processor = ViltProcessor.from_pretrained(\"/data/public/wbc/data_analysis/nsddata_voxel_analysis/things/ViLT/vilt-b32-finetuned-coco\")\n",
    "model = ViltModel.from_pretrained(\"/data/public/wbc/data_analysis/nsddata_voxel_analysis/things/ViLT/vilt-b32-finetuned-coco\").to(device)\n",
    "summary(model=model)\n",
    "result = np.array([])\n",
    "for nId, cId in tqdm(enumerate(cocoMap)):\n",
    "    cocoID = cId\n",
    "    image_file_name = imgIdToImg[cocoID][0][\"file_name\"] # 输入CocoID\n",
    "    url = imgDir+image_file_name\n",
    "    text = imgIdToCaps_simple[cocoID]\n",
    "    image = Image.open(url)\n",
    "    inputs = processor(image, text, return_tensors=\"pt\", truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(**inputs)\n",
    "    outputs = outputs.pooler_output.cpu().detach().numpy()\n",
    "    result = np.append(result, outputs)  \n",
    "\n",
    "result = result.reshape(73000,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(os.path.join(os.getcwd(), \"ViLT_embdding_{}.npy\").format(cap_lenght), result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('wbc_torch37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "fdf58fa4c4bcb9b02a8a74b7191137b3467d4fe406e2791b0c9082253e35a1f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
