# Brain_Encoding

We submitted an article on using machine learning technology for brain coding and related analysis to the AAAI 2023 conference.

Encoding models of brain activity provide new insights to understand how the brain represents real-world objects and to investigate the relations between biological and artificial neural networks. The majority of current neuroAI studies focused on the correspondence between the human visual system and computer vision models. We proposed that language-derived representation of natural scenes show more prominent effects than the perception knowledge and strongly engaged cortical regions beyond visual areas. Our results confirmed the coexistence of two distinct forms of knowledge representation of visual scenes in the human brain. 

Our main findings include: 1) compute vision models mostly captured neural activity in the visual system; 2) natural language models better encoded scene images in frontoparietal and attention networks; 3) multimodal fusion of image and text information highly boost knowledge representation in both visual and frontoparietal areas; 4) the inter-subject variability in scene understanding was dominant by the semantic component rather visual perception. Our study suggests that the neural representation of natural scenes was dominant by language rather than perception and may have a big impact on neuroAI and compute vision by seeing beyond the visual system.

Our method framework is roughly shown in the following figure.

![image](https://github.com/yzhlxg812/Brain_Encoding/assets/42958127/689ffb8c-b5bb-4ce4-8628-190345a6d6a9)

